# 迴歸(Regression)

我們大概瞭解了機器學習的概念, 還記得兩大學函數的方法嗎? 迴歸法和插值法! 現在直接切入實際操作!

來說說**迴歸**這件事, 我們給了一些點之後, 會想要找出最符合這些點的**函數**。

1. 製作假資料
	* 譬如說給一條線, f(x)=1.5x+0.8我們到Julia來定義這個函數f
		
			f(x) = 1.5x+0.8

	  有沒有發現, 真的太容易了, 平常怎麼寫、程式就怎麼寫!
	* 我們想要弄出一些點，最後模擬出這條線。準備50個點好了!
			
			x = linspace(0,10,50)
		`linspace(開始、結束、分割數) 用來分割`
	  
	  我們限制點的範圍是0到10,分割50份。
	* 來畫個圖, 畫圖請出套件PyPlot,
	
	       using PyPlot
	       plot(x, 1.5x+0.8)
	        
	       
	  ![plot_ch4_01](images/plot_ch4_01.png)
	* 對應的y是我們要學的, 但我們弄的這麼剛好的話, 怎麼學都像,
	所以我們給他一些擾動。
			
			y = 1.5x + 0.8 + 0.5randn(50)
	
	`randn(數目) 隨機產生平均數為0,標準差為1的array. 數目為array大小, 也可以不只一維`
	
	
   * 把我們剛剛取的點, 附上完全正確函數
	
			scatter(x,y)
			plot(x,f(x))
	 ![plot_ch4_02.png](images/plot_ch4_02.png)
	`scatter(x,y) 把點點出來`

2. 迴歸
	
   我們可以預先想好這些資料長得像什麼樣的函數, 我們都可以迴歸。在我們的例子, 做線性很容易。我們先做一個目標函數
   f(x)=ax+b
	 
	

		
	 



##最小平方法

